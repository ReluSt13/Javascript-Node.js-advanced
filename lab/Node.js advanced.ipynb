{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node.js advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'use strict';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event Emitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is useful to decouple event producers from event consumers. The API is quite simple and relies on two major methods:\n",
    "* ```emit``` is used to trigger an event\n",
    "* ```on``` is used to add a callback function that's going to be executed when the event is triggered\n",
    "\n",
    "Other useful methods are:\n",
    "* ```once()```: add a one-time listener\n",
    "* ```removeListener()``` / ```off()```: remove an event listener from an event\n",
    "* ```removeAllListeners()```: remove all listeners for an event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const EventEmitter = require('events');\n",
    "\n",
    "class TicketManager extends EventEmitter {\n",
    "\n",
    "    constructor(supply) {\n",
    "        super();\n",
    "        this.supply = supply;\n",
    "    }\n",
    "\n",
    "    buy(email, price) {\n",
    "        \n",
    "        if (this.supply == 0) {\n",
    "            this.emit('error', new Error('There are no more tickets left to purchase'));\n",
    "            return;\n",
    "        }\n",
    "\n",
    "        this.supply--;\n",
    "        this.emit('buy', email, price, Date.now());\n",
    "    }\n",
    "}\n",
    "\n",
    "const ticketManager = new TicketManager(10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmailService {\n",
    "    send(email) {\n",
    "        console.log(`Sending email to ${email}`);\n",
    "    }\n",
    "}\n",
    "\n",
    "const emailService = new EmailService();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticketManager.on('buy', (email, price, timestamp) => {\n",
    "    emailService.send(email);\n",
    "});\n",
    "\n",
    "ticketManager.on('error', (error) => {\n",
    "    console.error(`Handle error: ${error}`);\n",
    "});\n",
    "\n",
    "ticketManager.buy('test@email.com', 10);\n",
    "for (let i = 0; i < 10; ++i) {\n",
    "    ticketManager.buy(`test${i}@email.com`, 10);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streams are a way to handle file reading / writing, network communication etc in chunks of data processed piece by piece, without keeping the whole data in memory.\n",
    "\n",
    "e.g. Netflix doesn't ask you to wait until 5GB of video is downloaded in order to watch a movie, instead it sends to the video player a continuous stream of chunks, allowing the player to start as soon as it has filled its buffer with enough chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four major types of streams in Node.js:\n",
    "\n",
    "* ```Writable```: streams to which we can write data. e.g. ```fs.createWriteStream()``` lets us write data to a file using streams.\n",
    "\n",
    "* ```Readable```: streams from which data can be read. e.g. ```fs.createReadStream()``` lets us read the contents of a file.\n",
    "\n",
    "* ```Duplex```: streams that are both ```Readable``` and ```Writable```. e.g., ```net.Socket```\n",
    "\n",
    "* ```Transform```: streams that can modify or transform the data as it is written and read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const fs = require('fs');\n",
    "let data = '';\n",
    "\n",
    "const readable = fs.createReadStream('./files/file.txt', { encoding: 'utf8' });\n",
    "\n",
    "readable.on('data', (chunk) => {\n",
    "   data += chunk;\n",
    "});\n",
    "\n",
    "readable.on('end', () => {\n",
    "   console.log(data);\n",
    "});\n",
    "\n",
    "readable.on('error', (err) => {\n",
    "   console.error(err);\n",
    "});\n",
    "\n",
    "console.log('waiting for events ..');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although Stream API usage is pretty straight-forward, implementing one is not that simple. Especially, a Transform stream. \n",
    "\n",
    "Whenever you have a producer and (possibly multiple) consumer(s) you will face data flow control problems.\n",
    "\n",
    "Namely, backpressure is the common term used when the buffer used to store received data fills much faster than it is consumed, usually because the processing performed on that data takes too much.\n",
    "\n",
    "Luckily, Node.js comes with a solution to it. The Writable stream rejects any writes if its buffer is full and only accepts new writes after it emits 'drain' event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const { once } = require('events');\n",
    "const { finished } = require('stream/promises');\n",
    "\n",
    "async function writeIterableToFile(iterable, filePath) {\n",
    "\n",
    "    const writable = fs.createWriteStream(filePath, { encoding: 'utf8' });\n",
    "    for await (const chunk of iterable) {\n",
    "\n",
    "        if (!writable.write(chunk)) {\n",
    "            await once(writable, 'drain'); // Handle backpressure\n",
    "        }\n",
    "    }\n",
    "    writable.end();\n",
    "    await finished(writable); // Wait until done. Throws if there are errors.\n",
    "}\n",
    "\n",
    "const readableStream = fs.createReadStream('./files/file.txt', { encoding: 'utf8' });\n",
    "\n",
    "(async () => {\n",
    "\n",
    "    try {\n",
    "        \n",
    "        await writeIterableToFile(readableStream, './files/rewriten.txt');\n",
    "        console.info('Finished rewriting');\n",
    "        \n",
    "    } catch (err) {\n",
    "        console.error(err);\n",
    "    }\n",
    "\n",
    "})();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform streams are very useful if you want to transform data on the fly.\n",
    "\n",
    "e.g. you want to process an image uploaded to the server (tag people in it) before writing it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const zlib = require('zlib');\n",
    "\n",
    "const input = fs.createReadStream('./files/file.txt');\n",
    "const output = fs.createWriteStream('./files/file-compressed.txt.gz');\n",
    "\n",
    "input\n",
    "    .pipe(zlib.createGzip())\n",
    "    .pipe(output);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JavaScript (Node.js)",
   "language": "javascript",
   "name": "javascript"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "application/javascript",
   "name": "javascript",
   "version": "18.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
